[2025-11-25 11:53:49 root] (main_duquant_gptq.py 427): INFO Namespace(model='GSAI-ML/LLaDA-8B-Base', cache_dir='./cache', output_dir='./duquant_gptq_log/LLaDA-8B-Base_w4a4', save_dir=None, resume=None, calib_dataset='wikitext2', test_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa', eval_ppl=False, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_alpha=0.8, act_group_size=None, let_lr=0.005, smooth_lr=0.0001, lwc_lr=0.01, wd=0, epochs=0, smooth_epochs=0, smooth=True, let=False, lwc=False, aug_loss=False, symmetric=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=False, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, max_rotation_step=256, permutation_times=1, lac=0.9, swc=0.8, block_size=128, mxfp4_block_size=32, mmlu_data_dir='./mmlu/data', eval_mmlu=False, eval_mtbench=False, bench_name='mt_bench', question_begin=None, question_end=None, answer_file=None, max_new_token=1024, num_choices=1, num_gpus_per_model=1, num_gpus_total=1, max_gpu_memory=None, dtype=None, revision='main', quant_method='mxfp4', steps=1024, mc_num=128, gen_length=1024, block_length=1024, max_new_tokens=128, diffusion_steps=512, get_wa=False, use_gptq=False)
[2025-11-25 11:53:49 accelerate.utils.other] (other.py 492): WARNING Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2025-11-25 11:54:34 root] (main_duquant_gptq.py 539): INFO === Step 1: Apply DuQuant rotation and permutation ===
[2025-11-25 11:58:16 root] (duquant.py 49): INFO Starting ...
[2025-11-25 11:58:18 root] (duquant.py 178): INFO No attention mask caught from the first layer. Seems that model's attention works without a mask.
[2025-11-25 11:58:18 root] (duquant.py 203): INFO === Start quantize layer 0 ===
[2025-11-25 11:58:35 root] (duquant.py 203): INFO === Start quantize layer 1 ===
[2025-11-25 11:58:52 root] (duquant.py 203): INFO === Start quantize layer 2 ===
[2025-11-25 11:59:08 root] (duquant.py 203): INFO === Start quantize layer 3 ===
[2025-11-25 11:59:26 root] (duquant.py 203): INFO === Start quantize layer 4 ===
[2025-11-25 11:59:42 root] (duquant.py 203): INFO === Start quantize layer 5 ===
[2025-11-25 11:59:59 root] (duquant.py 203): INFO === Start quantize layer 6 ===
[2025-11-25 12:00:16 root] (duquant.py 203): INFO === Start quantize layer 7 ===
[2025-11-25 12:00:32 root] (duquant.py 203): INFO === Start quantize layer 8 ===
[2025-11-25 12:00:49 root] (duquant.py 203): INFO === Start quantize layer 9 ===
[2025-11-25 12:01:05 root] (duquant.py 203): INFO === Start quantize layer 10 ===
[2025-11-25 12:01:22 root] (duquant.py 203): INFO === Start quantize layer 11 ===
[2025-11-25 12:01:39 root] (duquant.py 203): INFO === Start quantize layer 12 ===
[2025-11-25 12:01:55 root] (duquant.py 203): INFO === Start quantize layer 13 ===
[2025-11-25 12:02:12 root] (duquant.py 203): INFO === Start quantize layer 14 ===
[2025-11-25 12:02:28 root] (duquant.py 203): INFO === Start quantize layer 15 ===
[2025-11-25 12:02:45 root] (duquant.py 203): INFO === Start quantize layer 16 ===
[2025-11-25 12:03:01 root] (duquant.py 203): INFO === Start quantize layer 17 ===
[2025-11-25 12:03:18 root] (duquant.py 203): INFO === Start quantize layer 18 ===
[2025-11-25 12:03:35 root] (duquant.py 203): INFO === Start quantize layer 19 ===
[2025-11-25 12:03:51 root] (duquant.py 203): INFO === Start quantize layer 20 ===
[2025-11-25 12:04:08 root] (duquant.py 203): INFO === Start quantize layer 21 ===
[2025-11-25 12:04:24 root] (duquant.py 203): INFO === Start quantize layer 22 ===
[2025-11-25 12:04:41 root] (duquant.py 203): INFO === Start quantize layer 23 ===
[2025-11-25 12:04:58 root] (duquant.py 203): INFO === Start quantize layer 24 ===
[2025-11-25 12:05:14 root] (duquant.py 203): INFO === Start quantize layer 25 ===
[2025-11-25 12:05:30 root] (duquant.py 203): INFO === Start quantize layer 26 ===
[2025-11-25 12:05:47 root] (duquant.py 203): INFO === Start quantize layer 27 ===
[2025-11-25 12:06:03 root] (duquant.py 203): INFO === Start quantize layer 28 ===
[2025-11-25 12:06:19 root] (duquant.py 203): INFO === Start quantize layer 29 ===
[2025-11-25 12:06:36 root] (duquant.py 203): INFO === Start quantize layer 30 ===
[2025-11-25 12:06:53 root] (duquant.py 203): INFO === Start quantize layer 31 ===
[2025-11-25 12:07:15 root] (main_duquant_gptq.py 572): INFO DuQuant rotation completed in 761.12 seconds
[2025-11-25 12:12:51 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 189): INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
[2025-11-25 12:12:51 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 243): INFO Using pre-initialized model
[2025-11-25 12:14:18 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of piqa from None to 0
[2025-11-25 12:14:18 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for piqa on rank 0...
[2025-11-25 12:14:45 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 559): INFO Running loglikelihood requests
[2025-11-25 12:22:43 datasets.fingerprint] (fingerprint.py 298): WARNING Parameter 'function'=<function LLaDAEvalHarness.loglikelihood.<locals>._tokenize at 0x7f0664c15ab0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
