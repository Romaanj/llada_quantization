[2025-11-25 13:07:03 root] (main_duquant_gptq.py 427): INFO Namespace(model='GSAI-ML/LLaDA-8B-Base', cache_dir='./cache', output_dir='./duquant_gptq_log/LLaDA-8B-Base_w4a4', save_dir=None, resume=None, calib_dataset='wikitext2', test_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='mmlu', eval_ppl=False, num_fewshot=5, wbits=4, abits=4, group_size=None, alpha=0.5, let_alpha=0.8, act_group_size=None, let_lr=0.005, smooth_lr=0.0001, lwc_lr=0.01, wd=0, epochs=0, smooth_epochs=0, smooth=True, let=False, lwc=False, aug_loss=False, symmetric=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=False, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, max_rotation_step=256, permutation_times=1, lac=0.9, swc=0.8, block_size=128, mxfp4_block_size=32, mmlu_data_dir='./mmlu/data', eval_mmlu=False, eval_mtbench=False, bench_name='mt_bench', question_begin=None, question_end=None, answer_file=None, max_new_token=1024, num_choices=1, num_gpus_per_model=1, num_gpus_total=1, max_gpu_memory=None, dtype=None, revision='main', quant_method='mxfp4', steps=1024, mc_num=1, gen_length=1024, block_length=1024, max_new_tokens=128, diffusion_steps=512, get_wa=False, use_gptq=False)
[2025-11-25 13:07:03 accelerate.utils.other] (other.py 492): WARNING Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2025-11-25 13:08:09 root] (main_duquant_gptq.py 539): INFO === Step 1: Apply DuQuant rotation and permutation ===
[2025-11-25 13:08:09 root] (main_duquant_gptq.py 546): INFO load calibration from ./cache/dataloader_LLaDA_wikitext2_128.cache
[2025-11-25 13:08:10 root] (duquant.py 49): INFO Starting ...
[2025-11-25 13:08:14 root] (duquant.py 178): INFO No attention mask caught from the first layer. Seems that model's attention works without a mask.
[2025-11-25 13:08:14 root] (duquant.py 203): INFO === Start quantize layer 0 ===
[2025-11-25 13:08:38 root] (duquant.py 203): INFO === Start quantize layer 1 ===
[2025-11-25 13:08:54 root] (duquant.py 203): INFO === Start quantize layer 2 ===
[2025-11-25 13:09:12 root] (duquant.py 203): INFO === Start quantize layer 3 ===
[2025-11-25 13:09:30 root] (duquant.py 203): INFO === Start quantize layer 4 ===
[2025-11-25 13:09:48 root] (duquant.py 203): INFO === Start quantize layer 5 ===
[2025-11-25 13:10:06 root] (duquant.py 203): INFO === Start quantize layer 6 ===
[2025-11-25 13:10:23 root] (duquant.py 203): INFO === Start quantize layer 7 ===
[2025-11-25 13:10:41 root] (duquant.py 203): INFO === Start quantize layer 8 ===
[2025-11-25 13:10:57 root] (duquant.py 203): INFO === Start quantize layer 9 ===
[2025-11-25 13:11:13 root] (duquant.py 203): INFO === Start quantize layer 10 ===
[2025-11-25 13:11:31 root] (duquant.py 203): INFO === Start quantize layer 11 ===
[2025-11-25 13:11:49 root] (duquant.py 203): INFO === Start quantize layer 12 ===
[2025-11-25 13:12:07 root] (duquant.py 203): INFO === Start quantize layer 13 ===
[2025-11-25 13:12:24 root] (duquant.py 203): INFO === Start quantize layer 14 ===
[2025-11-25 13:12:43 root] (duquant.py 203): INFO === Start quantize layer 15 ===
[2025-11-25 13:12:59 root] (duquant.py 203): INFO === Start quantize layer 16 ===
[2025-11-25 13:13:16 root] (duquant.py 203): INFO === Start quantize layer 17 ===
[2025-11-25 13:13:33 root] (duquant.py 203): INFO === Start quantize layer 18 ===
[2025-11-25 13:13:51 root] (duquant.py 203): INFO === Start quantize layer 19 ===
[2025-11-25 13:14:07 root] (duquant.py 203): INFO === Start quantize layer 20 ===
[2025-11-25 13:14:24 root] (duquant.py 203): INFO === Start quantize layer 21 ===
[2025-11-25 13:14:42 root] (duquant.py 203): INFO === Start quantize layer 22 ===
[2025-11-25 13:15:01 root] (duquant.py 203): INFO === Start quantize layer 23 ===
[2025-11-25 13:15:18 root] (duquant.py 203): INFO === Start quantize layer 24 ===
[2025-11-25 13:15:36 root] (duquant.py 203): INFO === Start quantize layer 25 ===
[2025-11-25 13:15:52 root] (duquant.py 203): INFO === Start quantize layer 26 ===
[2025-11-25 13:16:10 root] (duquant.py 203): INFO === Start quantize layer 27 ===
[2025-11-25 13:16:27 root] (duquant.py 203): INFO === Start quantize layer 28 ===
[2025-11-25 13:16:44 root] (duquant.py 203): INFO === Start quantize layer 29 ===
[2025-11-25 13:17:03 root] (duquant.py 203): INFO === Start quantize layer 30 ===
[2025-11-25 13:17:22 root] (duquant.py 203): INFO === Start quantize layer 31 ===
[2025-11-25 13:17:45 root] (main_duquant_gptq.py 572): INFO DuQuant rotation completed in 576.28 seconds
[2025-11-25 13:24:11 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 189): INFO Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
[2025-11-25 13:24:11 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 243): INFO Using pre-initialized model
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_machine_learning from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_college_physics from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_college_chemistry from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_college_mathematics from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_high_school_physics from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_high_school_chemistry from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_elementary_mathematics from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_computer_security from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_high_school_computer_science from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_high_school_statistics from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_college_biology from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_anatomy from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_electrical_engineering from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_high_school_mathematics from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_college_computer_science from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_conceptual_physics from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_astronomy from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_abstract_algebra from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_high_school_biology from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_marketing from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_human_aging from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_clinical_knowledge from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_professional_medicine from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_professional_accounting from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_medical_genetics from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_global_facts from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_miscellaneous from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_management from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_virology from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_business_ethics from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_college_medicine from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_nutrition from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_high_school_geography from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_sociology from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_high_school_macroeconomics from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_high_school_microeconomics from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_human_sexuality from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_high_school_government_and_politics from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_public_relations from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_professional_psychology from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_security_studies from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_us_foreign_policy from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_econometrics from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_high_school_psychology from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_international_law from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_world_religions from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_high_school_world_history from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_philosophy from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_jurisprudence from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_formal_logic from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_logical_fallacies from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_high_school_european_history from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_prehistory from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_moral_scenarios from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_professional_law from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_moral_disputes from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 309): WARNING Overwriting default num_fewshot of mmlu_high_school_us_history from None to 5
[2025-11-25 13:38:38 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_machine_learning on rank 0...
[2025-11-25 13:38:54 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_college_physics on rank 0...
[2025-11-25 13:39:07 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_college_chemistry on rank 0...
[2025-11-25 13:39:21 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_college_mathematics on rank 0...
[2025-11-25 13:39:34 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_high_school_physics on rank 0...
[2025-11-25 13:39:55 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_high_school_chemistry on rank 0...
[2025-11-25 13:40:22 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_elementary_mathematics on rank 0...
[2025-11-25 13:41:13 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_computer_security on rank 0...
[2025-11-25 13:41:26 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_high_school_computer_science on rank 0...
[2025-11-25 13:41:39 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_high_school_statistics on rank 0...
[2025-11-25 13:42:09 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_college_biology on rank 0...
[2025-11-25 13:42:28 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_anatomy on rank 0...
[2025-11-25 13:42:46 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_electrical_engineering on rank 0...
[2025-11-25 13:43:05 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_high_school_mathematics on rank 0...
[2025-11-25 13:43:42 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_college_computer_science on rank 0...
[2025-11-25 13:43:55 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_conceptual_physics on rank 0...
[2025-11-25 13:44:27 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_astronomy on rank 0...
[2025-11-25 13:44:47 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_abstract_algebra on rank 0...
[2025-11-25 13:45:00 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_high_school_biology on rank 0...
[2025-11-25 13:45:42 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_marketing on rank 0...
[2025-11-25 13:46:13 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_human_aging on rank 0...
[2025-11-25 13:46:43 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_clinical_knowledge on rank 0...
[2025-11-25 13:47:20 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_professional_medicine on rank 0...
[2025-11-25 13:47:56 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_professional_accounting on rank 0...
[2025-11-25 13:48:34 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_medical_genetics on rank 0...
[2025-11-25 13:48:48 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_global_facts on rank 0...
[2025-11-25 13:49:01 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_miscellaneous on rank 0...
[2025-11-25 13:50:46 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_management on rank 0...
[2025-11-25 13:51:00 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_virology on rank 0...
[2025-11-25 13:51:23 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_business_ethics on rank 0...
[2025-11-25 13:51:36 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_college_medicine on rank 0...
[2025-11-25 13:51:59 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_nutrition on rank 0...
[2025-11-25 13:52:40 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_high_school_geography on rank 0...
[2025-11-25 13:53:07 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_sociology on rank 0...
[2025-11-25 13:53:34 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_high_school_macroeconomics on rank 0...
[2025-11-25 13:54:26 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_high_school_microeconomics on rank 0...
[2025-11-25 13:54:58 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_human_sexuality on rank 0...
[2025-11-25 13:55:16 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_high_school_government_and_politics on rank 0...
[2025-11-25 13:55:42 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_public_relations on rank 0...
[2025-11-25 13:55:56 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_professional_psychology on rank 0...
[2025-11-25 13:57:18 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_security_studies on rank 0...
[2025-11-25 13:57:51 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_us_foreign_policy on rank 0...
[2025-11-25 13:58:05 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_econometrics on rank 0...
[2025-11-25 13:58:20 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_high_school_psychology on rank 0...
[2025-11-25 13:59:33 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_international_law on rank 0...
[2025-11-25 13:59:49 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_world_religions on rank 0...
[2025-11-25 14:00:12 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_high_school_world_history on rank 0...
[2025-11-25 14:00:44 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_philosophy on rank 0...
[2025-11-25 14:01:26 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_jurisprudence on rank 0...
[2025-11-25 14:01:41 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_formal_logic on rank 0...
[2025-11-25 14:01:58 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_logical_fallacies on rank 0...
[2025-11-25 14:02:20 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_high_school_european_history on rank 0...
[2025-11-25 14:02:42 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_prehistory on rank 0...
[2025-11-25 14:03:26 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_moral_scenarios on rank 0...
[2025-11-25 14:05:27 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_professional_law on rank 0...
[2025-11-25 14:08:54 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_moral_disputes on rank 0...
[2025-11-25 14:09:41 lm_evaluation_harness.lm_eval.api.task] (task.py 434): INFO Building contexts for mmlu_high_school_us_history on rank 0...
[2025-11-25 14:10:08 lm_evaluation_harness.lm_eval.evaluator] (evaluator.py 559): INFO Running loglikelihood requests
[2025-11-25 14:18:17 datasets.fingerprint] (fingerprint.py 298): WARNING Parameter 'function'=<function LLaDAEvalHarness.loglikelihood.<locals>._tokenize at 0x7f8721eb92d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
